import streamlit as st
import pandas as pd
from vector_store import KnowledgeBase
from aliyun_embedder import AliYunEmbedder
from doubao_ai import DoubaoAI
from file_processor import process_uploaded_file
import os
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ - é˜¿é‡Œäº‘ & è±†åŒ…é›†æˆ",
    page_icon="ğŸ“š",
    layout="wide"
)

# åˆå§‹åŒ–çŸ¥è¯†åº“å’ŒAIå®¢æˆ·ç«¯
@st.cache_resource
def init_knowledge_base():
    # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ä½œä¸ºå¤‡é€‰
    aliyun_api_key = os.getenv("ALIYUN_API_KEY", "sk-8a1b33b774344ba98cf22fc660b6c9a2")
    doubao_api_key = os.getenv("DOUBAO_API_KEY", "af304f26-0164-4318-84e7-d70ac67f2e07")
    
    # åˆå§‹åŒ–åµŒå…¥å™¨
    embedder = AliYunEmbedder(api_key=aliyun_api_key)
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    kb = KnowledgeBase(embedder=embedder)
    
    # åˆå§‹åŒ–è±†åŒ…AI
    doubao = DoubaoAI(api_key=doubao_api_key)
    
    return kb, doubao

# åŠ è½½ç¤ºä¾‹æ•°æ®
def load_sample_data():
    """åŠ è½½ç¤ºä¾‹çŸ¥è¯†åº“æ•°æ®"""
    sample_data = [
        {
            "text": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹",
            "category": "æœºå™¨å­¦ä¹ ",
            "source": "AIåŸºç¡€çŸ¥è¯†"
        },
        {
            "text": "æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œï¼Œèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡ç‰¹å¾",
            "category": "æ·±åº¦å­¦ä¹ ", 
            "source": "ç¥ç»ç½‘ç»œåŸºç¡€"
        },
        {
            "text": "è‡ªç„¶è¯­è¨€å¤„ç†ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€",
            "category": "NLP",
            "source": "è¯­è¨€å¤„ç†æŠ€æœ¯"
        },
        {
            "text": "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿç†è§£å’Œåˆ†æè§†è§‰ä¿¡æ¯",
            "category": "CV",
            "source": "å›¾åƒå¤„ç†æŠ€æœ¯"
        },
        {
            "text": "å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜å†³ç­–ç­–ç•¥",
            "category": "å¼ºåŒ–å­¦ä¹ ",
            "source": "å†³ç­–ç®—æ³•"
        },
        {
            "text": "Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•",
            "category": "NLP",
            "source": "ç°ä»£æ¶æ„"
        },
        {
            "text": "å·ç§¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²",
            "category": "CV",
            "source": "ç¥ç»ç½‘ç»œåº”ç”¨"
        },
        {
            "text": "BERTæ¨¡å‹é€šè¿‡åŒå‘ç¼–ç å™¨å®ç°æ›´å¥½çš„è¯­è¨€ç†è§£",
            "category": "NLP", 
            "source": "é¢„è®­ç»ƒæ¨¡å‹"
        },
        {
            "text": "ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå¯ä»¥ç”Ÿæˆé€¼çœŸçš„å›¾åƒå’Œæ•°æ®",
            "category": "ç”Ÿæˆæ¨¡å‹",
            "source": "åˆ›é€ æ€§AI"
        },
        {
            "text": "çŸ¥è¯†å›¾è°±å°†ä¿¡æ¯ç»„ç»‡æˆç»“æ„åŒ–çš„è¯­ä¹‰ç½‘ç»œ",
            "category": "çŸ¥è¯†è¡¨ç¤º",
            "source": "æ•°æ®ç»“æ„"
        }
    ]
    return sample_data

def main():
    st.title("ğŸ“š æ™ºèƒ½çŸ¥è¯†åº“ç³»ç»Ÿ - é˜¿é‡Œäº‘ & è±†åŒ…é›†æˆ")
    st.markdown("åŸºäºé˜¿é‡Œäº‘text-embedding-v4å’Œè±†åŒ…AIçš„è¯­ä¹‰æœç´¢çŸ¥è¯†åº“")
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“å’ŒAIå®¢æˆ·ç«¯
    kb, doubao = init_knowledge_base()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ç³»ç»Ÿè®¾ç½®")
        
        # APIå¯†é’¥è®¾ç½®
        st.subheader("APIé…ç½®")
        aliyun_api_key = st.text_input("é˜¿é‡Œäº‘API Key:", value=os.getenv("ALIYUN_API_KEY", "sk-8a1b33b774344ba98cf22fc660b6c9a2"), type="password")
        doubao_api_key = st.text_input("è±†åŒ…API Key:", value=os.getenv("DOUBAO_API_KEY", "af304f26-0164-4318-84e7-d70ac67f2e07"), type="password")
        doubao_model_id = st.text_input("è±†åŒ…æ¨¡å‹ID:", value="ep-20250714212604-xmv9d")
        
        if st.button("ğŸ”„ åº”ç”¨APIè®¾ç½®"):
            os.environ["ALIYUN_API_KEY"] = aliyun_api_key
            os.environ["DOUBAO_API_KEY"] = doubao_api_key
            st.cache_resource.clear()
            st.rerun()
        
        st.divider()
        
        # çŸ¥è¯†åº“ç»Ÿè®¡
        st.subheader("çŸ¥è¯†åº“ç»Ÿè®¡")
        if hasattr(kb.vector_store, 'texts'):
            st.write(f"æ–‡æ¡£æ•°é‡: {len(kb.vector_store.texts)}")
            st.write(f"å‘é‡ç»´åº¦: {kb.vector_store.embedding_dim}")
        
        st.divider()
        
        # æ·»åŠ æ–‡æ¡£åŒºåŸŸ
        st.subheader("æ·»åŠ æ–°æ–‡æ¡£")
        new_text = st.text_area("è¾“å…¥æ–‡æ¡£å†…å®¹:", height=100)
        category = st.text_input("åˆ†ç±»æ ‡ç­¾:")
        source = st.text_input("æ¥æº:")
        
        if st.button("ğŸ“ æ·»åŠ æ–‡æ¡£") and new_text:
            metadata = {"category": category, "source": source}
            kb.add_documents([new_text], [metadata])
            st.success("æ–‡æ¡£æ·»åŠ æˆåŠŸ!")
            time.sleep(1)
            st.rerun()
        
        st.divider()
        
        # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
        st.subheader("æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader("é€‰æ‹©DOCXæˆ–XLSXæ–‡ä»¶", type=["docx", "xlsx"])
        file_category = st.text_input("æ–‡ä»¶åˆ†ç±»æ ‡ç­¾:", key="file_category")
        file_source = st.text_input("æ–‡ä»¶æ¥æº:", key="file_source")
        
        if uploaded_file and st.button("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶"):
            file_ext = uploaded_file.name.split('.')[-1].lower()
            if file_ext in ['docx', 'xlsx']:
                with st.spinner("å¤„ç†æ–‡ä»¶ä¸­..."):
                    try:
                        texts = process_uploaded_file(uploaded_file, file_ext)
                        if texts:
                            metadata = {"category": file_category, "source": file_source}
                            kb.add_documents(texts, [metadata] * len(texts))
                            st.success(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼æå–äº† {len(texts)} æ¡æ–‡æœ¬")
                            time.sleep(1)
                            st.rerun()
                        else:
                            st.warning("æ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                    except Exception as e:
                        st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            else:
                st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ DOCXæˆ–XLSXæ–‡ä»¶")
    
    # ä¸»å†…å®¹åŒº
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ” è¯­ä¹‰æœç´¢", "ğŸ¤– æ™ºèƒ½é—®ç­”", "ğŸ“Š çŸ¥è¯†åº“ç®¡ç†", "â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"])
    
    with tab1:
        st.header("è¯­ä¹‰æœç´¢")
        
        # æœç´¢å‚æ•°
        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input("è¾“å…¥æœç´¢æŸ¥è¯¢:", placeholder="ä¾‹å¦‚: æœºå™¨å­¦ä¹ ç®—æ³•", key="search_query")
        with col2:
            top_k = st.slider("è¿”å›ç»“æœæ•°:", 1, 10, 5, key="search_top_k")
            similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼:", 0.0, 1.0, 0.6, 0.1, key="search_threshold")
        
        if query:
            with st.spinner("æ­£åœ¨æœç´¢..."):
                results = kb.search(query, top_k=top_k, similarity_threshold=similarity_threshold)
            
            if results:
                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
                
                for i, result in enumerate(results, 1):
                    with st.expander(f"ç»“æœ #{i} - ç›¸ä¼¼åº¦: {result['similarity']:.3f}"):
                        st.write(f"**å†…å®¹:** {result['text']}")
                        if result['metadata']:
                            st.write(f"**åˆ†ç±»:** {result['metadata'].get('category', 'æ— ')}")
                            st.write(f"**æ¥æº:** {result['metadata'].get('source', 'æ— ')}")
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·å°è¯•è°ƒæ•´æœç´¢æ¡ä»¶")
    
    with tab2:
        st.header("æ™ºèƒ½é—®ç­”")
        st.info("åŸºäºçŸ¥è¯†åº“å†…å®¹çš„æ™ºèƒ½é—®ç­”ï¼Œä½¿ç”¨è±†åŒ…AIç”Ÿæˆå›ç­”")
        
        # é—®ç­”å‚æ•°
        qa_query = st.text_input("è¾“å…¥æ‚¨çš„é—®é¢˜:", placeholder="ä¾‹å¦‚: æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ", key="qa_query")
        qa_top_k = st.slider("æ£€ç´¢æ–‡æ¡£æ•°:", 1, 10, 3, key="qa_top_k")
        qa_threshold = st.slider("æ£€ç´¢é˜ˆå€¼:", 0.0, 1.0, 0.5, 0.1, key="qa_threshold")
        
        if st.button("ğŸ§  ç”Ÿæˆå›ç­”", key="generate_answer") and qa_query:
            with st.spinner("æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“å¹¶ç”Ÿæˆå›ç­”..."):
                # é¦–å…ˆæ£€ç´¢ç›¸å…³æ–‡æ¡£
                results = kb.search(qa_query, top_k=qa_top_k, similarity_threshold=qa_threshold)
                
                if results:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n".join([f"{i+1}. {result['text']}" for i, result in enumerate(results)])
                    
                    # ä½¿ç”¨è±†åŒ…AIç”Ÿæˆå›ç­”
                    try:
                        answer = doubao.knowledge_base_qa(
                            model=doubao_model_id,
                            query=qa_query,
                            context=context
                        )
                        
                        st.success("å›ç­”ç”Ÿæˆå®Œæˆ!")
                        st.markdown("### ğŸ’¡ AIå›ç­”:")
                        st.write(answer)
                        
                        st.markdown("### ğŸ“š å‚è€ƒå†…å®¹:")
                        for i, result in enumerate(results, 1):
                            with st.expander(f"å‚è€ƒæ–‡æ¡£ #{i} - ç›¸ä¼¼åº¦: {result['similarity']:.3f}"):
                                st.write(result['text'])
                                if result['metadata']:
                                    st.write(f"åˆ†ç±»: {result['metadata'].get('category', 'æ— ')}")
                                    st.write(f"æ¥æº: {result['metadata'].get('source', 'æ— ')}")
                    
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆå›ç­”")
    
    with tab3:
        st.header("çŸ¥è¯†åº“ç®¡ç†")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("åˆå§‹åŒ–çŸ¥è¯†åº“")
            if st.button("ğŸ“¥ åŠ è½½ç¤ºä¾‹æ•°æ®"):
                sample_data = load_sample_data()
                texts = [item["text"] for item in sample_data]
                metadata = [{"category": item["category"], "source": item["source"]} 
                           for item in sample_data]
                
                kb.add_documents(texts, metadata)
                st.success("ç¤ºä¾‹æ•°æ®åŠ è½½å®Œæˆ!")
                time.sleep(1)
                st.rerun()
            
            st.divider()
            
            st.subheader("å¯¼å‡ºçŸ¥è¯†åº“")
            if st.button("ğŸ’¾ ä¿å­˜çŸ¥è¯†åº“"):
                kb.save("my_knowledge_base")
                st.success("çŸ¥è¯†åº“å·²ä¿å­˜åˆ°æ–‡ä»¶!")
        
        with col2:
            st.subheader("å¯¼å…¥çŸ¥è¯†åº“")
            if st.button("ğŸ“¤ åŠ è½½çŸ¥è¯†åº“") and os.path.exists("my_knowledge_base.index"):
                kb.load("my_knowledge_base")
                st.success("çŸ¥è¯†åº“åŠ è½½å®Œæˆ!")
                time.sleep(1)
                st.rerun()
            elif not os.path.exists("my_knowledge_base.index"):
                st.info("æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„çŸ¥è¯†åº“æ–‡ä»¶")
    
    with tab4:
        st.header("ç³»ç»Ÿä¿¡æ¯")
        
        st.subheader("æŠ€æœ¯æ¶æ„")
        st.markdown("""
        - **åµŒå…¥æ¨¡å‹**: é˜¿é‡Œäº‘ text-embedding-v4 (æœ€é«˜2048ç»´)
        - **AIå¼•æ“**: è±†åŒ…å¤§æ¨¡å‹
        - **å‘é‡å­˜å‚¨**: FAISS é«˜æ•ˆç›¸ä¼¼åº¦æœç´¢
        - **å‰ç«¯ç•Œé¢**: Streamlit Webåº”ç”¨
        - **æœç´¢ç®—æ³•**: ä½™å¼¦ç›¸ä¼¼åº¦ + è¿‘ä¼¼æœ€è¿‘é‚»
        """)
        
        st.subheader("ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        1. åœ¨ä¾§è¾¹æ é…ç½®APIå¯†é’¥å’Œæ¨¡å‹ID
        2. æ·»åŠ æ–‡æ¡£æˆ–åŠ è½½ç¤ºä¾‹æ•°æ®æ„å»ºçŸ¥è¯†åº“
        3. ä½¿ç”¨è¯­ä¹‰æœç´¢æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£
        4. ä½¿ç”¨æ™ºèƒ½é—®ç­”è·å–AIç”Ÿæˆçš„å›ç­”
        5. ä½¿ç”¨ç®¡ç†åŠŸèƒ½ä¿å­˜/åŠ è½½çŸ¥è¯†åº“
        """)
        
        st.subheader("æ€§èƒ½æŒ‡æ ‡")
        if hasattr(kb.vector_store, 'index'):
            stats = kb.vector_store.get_stats()
            st.write(f"- æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
            st.write(f"- ç´¢å¼•å¤§å°: {stats['index_size']}")
            st.write(f"- å‘é‡ç»´åº¦: {stats['embedding_dim']}")

if __name__ == "__main__":
    main()